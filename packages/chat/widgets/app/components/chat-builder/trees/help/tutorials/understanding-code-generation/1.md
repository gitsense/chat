<!--
Component: Gemini 2.5 Flash Thinking Core Directives
Block-UUID: 1178c9a2-d70f-42fb-8fd3-0ef55f17cef9
Parent-UUID: N/A
Version: 1.0.0
Description: Core directives and protocols for Gemini 2.5 Flash Thinking, defining its behavior, code generation rules, and interaction patterns.
Language: Markdown (for document structure, but applies to all code languages)
Created-at: 2025-07-30T15:56:10.900Z
Authors: Gemini 2.5 Flash Thinking (v1.0.0)
-->


> GitSense Chat Help: Tutorials / Understanding Code Generation

## Understanding Code Generation

In this quick interactive guide, you'll learn how to:
*   **Generate code** with an LLM.
*   **Iterate on code** by refining your requests.
*   Understand how GitSense Chat provides **traceability** for every line of AI-generated code, whether it's a patch or full code.

Let's start! Your first task is to ask me to generate a simple program.

**Copy and paste following into the chat input below then press enter**

```text
Write a "Hello World" program in JavaScript
```

> **Pro Tip**: When writing complex instructions, turn off **"Send on enter"** and click the **up arrow** or drag the **grabber icon** up to make writing more comfortable.

### Potential Patch Error

Because diff generation is a challenging task for language models, the patch we return might not always apply cleanly. If you encounter a patch-error, just follow the provided instructions and we'll help you fix it or provide the complete updated file.

---hidden-split---

**LLM Guidance for "Understanding Code Generation: From Prompt to Output" Tutorial**

**Objective:** Guide the user through generating initial code, iterating on it, and understanding basic code traceability (UUIDs) and output handling (patches/full code).

**Internal State Tracking (Conceptual):**
*   `tutorial_stage`: `initial_prompt` (default), `awaiting_change_prompt`, `awaiting_output_review`
*   `first_hello_world_block_uuid`: (To store the UUID of the initial Hello World code)

**Initial State (`tutorial_stage: initial_prompt`):**
*   **Expected User Input:** `Write a "Hello World" program in JavaScript.` (or similar intent)
*   **If matches expected input:**
    1.  **Generate Code:** Provide a standard "Hello World" JavaScript program.
        *   Ensure the code block includes a *new* `Block-UUID` and `Parent-UUID: N/A` in its header.
        *   **Crucial:** Store this `Block-UUID` in `first_hello_world_block_uuid` for later use.
        *   Example:
            ```javascript
            /*
             * Component: Hello World Program
             * Block-UUID: 57475948-b2a4-4e9c-bf11-7c88e3f3ac82
             * Parent-UUID: N/A
             * Version: 1.0.0
             * Description: A simple "Hello World" program in JavaScript.
             * Language: JavaScript
             * Created-at: {{GS-TIMESTAMP}}
             * Authors: Gemini 2.5 Flash Thinking (v1.0.0)
             */


            function helloWorld() {
              console.log("Hello, World!");
            }

            helloWorld();
            ```
    2.  **Next Instruction & Traceability Explanation:** After the code block, provide a message.
        *   "Great! You've generated your first code. Notice the special header in the code block? The `Block-UUID` is a unique identifier for this exact piece of code. If this code ever makes it into your production environment, you can use this UUID to trace it right back to this conversation, understanding its origin and evolution. This is part of how GitSense Chat provides **unparalleled traceability** for all AI-generated code."
        *   "Now, let's iterate on this code and see how changes are tracked. For the next step, **copy and paste the exact command below into the chat input then press enter**:"
        *   ```text
            Now change 'Hello' to 'Hey'
            ```
        *   ---
        *   // Current state: awaiting_change_prompt (LLM internal note)
    3.  **Update Internal State:** `tutorial_stage` to `awaiting_change_prompt`.

*   **If input does NOT match expected input (e.g., user asks something else):**
    *   "That's an interesting question, but for this tutorial, let's stick to the script for now. Please try to type: `Write a "Hello World" program in JavaScript.`"
    *   // Current state: initial_prompt (LLM internal note)
    *   **Maintain Internal State:** `tutorial_stage` remains `initial_prompt`.

**State 2 (`tutorial_stage: awaiting_change_prompt`):**
*   **Expected User Input:** `Now change 'Hello' to 'Hey'.` (or similar intent)
*   **If matches expected input:**
    1.  **Generate Output (Patch or Full Code):** Attempt to generate either a patch or the full updated code.
        *   **Crucial Instruction for LLM:** When generating *either* a patch or full code, ensure the `Parent-UUID` in the *new* code block's metadata header points to the `first_hello_world_block_uuid` (the UUID of the initial "Hello World" code). This reinforces the lineage.
        *   Example Patch:
            ```diff
            # Patch Metadata
            # Source-Block-UUID: [FIRST_HELLO_WORLD_UUID]
            # Target-Block-UUID: 3aa50416-6df2-4ac1-9f55-111f1adb64f4
            # Source-Version: 1.0.0
            # Target-Version: 1.1.0
            # Description: Update greeting from 'Hello' to 'Hey'.
            # Authors: Gemini 2.5 Flash Thinking (v1.0.0), Gemini 2.5 Flash Thinking (v1.1.0)

            # --- PATCH START MARKER ---
            --- Original
            +++ Modified
            @@ -13,7 +13,7 @@
             function helloWorld() {
            -  console.log("Hello, World!");
            +  console.log("Hey, World!");
             }

             helloWorld();
            # --- PATCH END MARKER ---
            ```
        *   Example Full Code:
            ```javascript
            /*
             * Component: Hello World Program
             * Block-UUID: d5459b46-0616-47e0-979d-7e0e3293510b
             * Parent-UUID: [FIRST_HELLO_WORLD_UUID]
             * Version: 1.1.0
             * Description: A simple "Hello World" program in JavaScript.
             * Language: JavaScript
             * Created-at: {{GS-TIMESTAMP}}
             * Authors: Gemini 2.5 Flash Thinking (v1.0.0), Gemini 2.5 Flash Thinking (v1.1.0)
             */


            function helloWorld() {
              console.log("Hey, World!");
            }

            helloWorld();
            ```
    2.  **Instructions for Output Review:**
        *   "Great! I've updated the code for you. You will receive either a **code patch** (showing the exact changes) or the **full updated code**."
        *   "**If you see a patch:** This is the most common output for modifications. Look for the area above it. **If you see a 'Preview Changes' link:** This means the patch is valid! Click it to see a side-by-side comparison of the original and modified code. This is how you review AI-generated changes before applying them."
        *   "**If you see an 'Error' message and a 'Start Fix Patch Chat' button:** Don't worry! Sometimes LLMs generate imperfect patches. This button is your solution. Click it, and I'll guide you through correcting the patch or getting the full, corrected code."
        *   "**If you see the full updated code:** Notice its header! It now has a new `Block-UUID` and its `Parent-UUID` points back to the original 'Hello World' code, maintaining that crucial traceability."
        *   "This demonstrates how GitSense Chat helps you work with LLMs, ensuring you always get usable and traceable code, whether it's a patch or a full update."
        *   ---
        *   // Current state: awaiting_output_review (LLM internal note)
    3.  **End of Tutorial Segment:** "This concludes the 'Understanding Code Generation' quick tutorial. Feel free to experiment further, or ask me if you'd like to try another tutorial!"
    4.  **Update Internal State:** `tutorial_stage` to `awaiting_output_review`.

*   **If input does NOT match expected input:**
    *   "We're in the middle of updating the code. Please try to type: `Now change 'Hello' to 'Hey'.`"
    *   // Current state: awaiting_change_prompt (LLM internal note)
    *   **Maintain Internal State:** `tutorial_stage` remains `awaiting_change_prompt`.

**State 3 (`tutorial_stage: awaiting_output_review`):**
*   **User Action:** User interacts with the output (clicks "Preview Changes" or "Start Fix Patch Chat" or simply reviews the full code).
*   **LLM Response:** No direct response needed from the LLM unless the user asks a new question. The frontend handles the modal/new chat.
*   **If user asks a new question:** "You've completed the code generation tutorial! What would you like to explore next?"
